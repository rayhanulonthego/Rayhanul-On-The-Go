{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a4e8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 22:21:10.747732: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-23 22:21:10.747824: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-23 22:21:10.847766: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-23 22:21:11.041200: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-23 22:21:12.565235: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# LSTM Forecasting Pipeline (Final Version with CSV-safe Preprocessing)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04eec1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 1. Load Data\n",
    "# -------------------------\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'\n",
    "\n",
    "if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
    "    raise FileNotFoundError(\"Check if the train and test CSV paths are correct.\")\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d4d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 2. Preprocessing Pipeline\n",
    "# -------------------------\n",
    "def preprocess(df, is_train=True):\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['is_weekend'] = df['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    \n",
    "    if is_train:\n",
    "        store_dummies = pd.get_dummies(df['store'], prefix='store')\n",
    "        item_dummies = pd.get_dummies(df['item'], prefix='item')\n",
    "        df = pd.concat([df, store_dummies, item_dummies], axis=1)\n",
    "        df.drop(['store', 'item'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "train_df = preprocess(train_df, is_train=True)\n",
    "test_df = preprocess(test_df, is_train=True)  # Note: ensure columns match\n",
    "\n",
    "# Align columns between train and test\n",
    "train_cols = set(train_df.columns) - {'sales'}\n",
    "test_missing = list(train_cols - set(test_df.columns))\n",
    "for col in test_missing:\n",
    "    test_df[col] = 0\n",
    "\n",
    "test_df = test_df[[col for col in train_df.columns if col != 'sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af79e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. Prepare Sequences\n",
    "# -------------------------\n",
    "lookback = 30\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "train_features = train_df.drop(columns=['date', 'sales'])\n",
    "train_target = train_df['sales'].values.reshape(-1, 1)\n",
    "\n",
    "scaled_features = scaler_x.fit_transform(train_features)\n",
    "scaled_target = scaler_y.fit_transform(train_target)\n",
    "\n",
    "X_train, y_train = [], []\n",
    "for i in range(lookback, len(scaled_features)):\n",
    "    X_train.append(scaled_features[i - lookback:i])\n",
    "    y_train.append(scaled_target[i])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be2da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4. Build & Train LSTM Model\n",
    "# -------------------------\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.2, verbose=1)\n",
    "\n",
    "# -------------------------\n",
    "# 5. Predict on Test Data\n",
    "# -------------------------\n",
    "scaled_test = scaler_x.transform(test_df.drop(columns=['date']))\n",
    "\n",
    "X_test = []\n",
    "for i in range(lookback, len(scaled_test)):\n",
    "    X_test.append(scaled_test[i - lookback:i])\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "pred_scaled = model.predict(X_test)\n",
    "pred_sales = scaler_y.inverse_transform(pred_scaled).flatten()\n",
    "\n",
    "# Fill predictions into test_df\n",
    "test_result = test_df[lookback:].copy()\n",
    "test_result['sales_predicted'] = pred_sales\n",
    "\n",
    "# -------------------------\n",
    "# 6. Visualization\n",
    "# -------------------------\n",
    "# Sales over time for a sample store/item\n",
    "plt.figure(figsize=(15, 4))\n",
    "sample_df = train_df.copy()\n",
    "sample_df['date'] = pd.to_datetime(sample_df['date'])\n",
    "sample_df = sample_df.sort_values('date')\n",
    "plt.plot(sample_df['date'], train_target, label='Actual Sales')\n",
    "plt.title(\"Sales Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Validation Plot\n",
    "X_subtrain, X_val, y_subtrain, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "model.fit(X_subtrain, y_subtrain, epochs=2, batch_size=128, verbose=0)\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(scaler_y.inverse_transform(y_val[-100:]), label='Actual')\n",
    "plt.plot(scaler_y.inverse_transform(y_val_pred[-100:]), label='Predicted')\n",
    "plt.title(\"Validation: Actual vs Predicted Sales (last 100 points)\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature Correlation\n",
    "plt.figure(figsize=(8, 6))\n",
    "numeric_cols = ['year', 'month', 'day', 'dayofweek', 'is_weekend', 'sales']\n",
    "corr = train_df[numeric_cols].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prediction Distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(pred_sales, bins=50, kde=True)\n",
    "plt.title(\"Predicted Sales Distribution (Test Set)\")\n",
    "plt.xlabel(\"Predicted Sales\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Snapshot for a few Store-Item combinations (if dummies exist)\n",
    "store_cols = [col for col in test_result.columns if col.startswith('store_')]\n",
    "item_cols = [col for col in test_result.columns if col.startswith('item_')]\n",
    "\n",
    "if store_cols and item_cols:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    test_result['store_id'] = test_result[store_cols].idxmax(axis=1).apply(lambda x: x.split('_')[1])\n",
    "    test_result['item_id'] = test_result[item_cols].idxmax(axis=1).apply(lambda x: x.split('_')[1])\n",
    "    unique_pairs = test_result[['store_id', 'item_id']].drop_duplicates().head(4)\n",
    "    for i, row in unique_pairs.iterrows():\n",
    "        sub = test_result[(test_result['store_id'] == row['store_id']) & (test_result['item_id'] == row['item_id'])]\n",
    "        plt.bar(f\"Store {row['store_id']}, Item {row['item_id']}\", sub['sales_predicted'].mean())\n",
    "    plt.title(\"Predicted Sales for Selected Store-Item Pairs (Test)\")\n",
    "    plt.ylabel(\"Avg Predicted Sales\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
